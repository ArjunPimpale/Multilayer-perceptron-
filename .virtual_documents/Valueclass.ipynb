import matplotlib as plt
import math



class Value:

    def __init__(self,data,_children=(),_op = '',label = ''):
        self.data = data
        self._prev = set(_children)
        self._op = _op
        self.label = label
        self._backward = lambda:None
        self.grad = 0.0

    def __repr__(self):
        return f"Value(data = {self.data})"

    def __add__(self,other):
        if not isinstance(other, Value):
             other = Value(other)
        out = Value(self.data+other.data,(self,other),'+')
        def _backward():
            self.grad += 1.0*out.grad
            other.grad += 1.0*self.grad
        out._backward = _backward
        return out

    def __radd__(self,other):
        return self + other

    def __neg__(self):
        return self*(-1.0)

    def __sub__(self,other):
        return self + (-other)
    def __rsub__(self,other):
        return self - other
        
    def __mul__(self,other):
        if not isinstance(other, Value):
             other = Value(other)
             
                
        out = Value(self.data*other.data,(self,other),'*')
        def _backward():
            self.grad += other.data*out.grad
            other.grad += self.data*out.grad

        out._backward = _backward
        return out
        
    def __pow__(self,other):
        assert isinstance(other,(int,float)), 'only supports int/float'
        x = self.data ** other
        out = Value(x,(self,),'**')
        def _backward():
            self.grad = other * (self.data **(other - 1))*out.grad

        out._backward = _backward
        return out
        
        
    def __truediv__(self,other):
        return self*(other**-1)
        
    def __rmul__(self,other):
        return self*other

    def exp(self):
        x = self.data
        out = Value(math.exp(x),(self,),'exp')
        def _backward():
            self.grad += out.data * out.grad
        out._backward = _backward
        return out
        
    def tanh(self):
        x = self.data
        t = (math.exp(2*x)-1)/(math.exp(2*x)+1)
        out = Value(t,(self,),'tanh')
        def _backward():
            self.grad += (1-t**2)*out.grad

        out._backward = _backward
        return out

    def backward(self):
                # MY ATTEMPT logic...
        # def backpropogation(L,arr = []):
        #     for i in L._prev:
        #         backpropogation(i,arr)
        
        #     arr.append(L)
        
        
        # def backprop(L):
        #     arr = []
        #     L.grad = 1
        #     backpropogation(L,arr)
        #     print(arr)
        #     while arr:
        #         ele = arr[-1]
        #         ele._backward()
        #         arr.pop()
        topo = []
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)

                topo.append(v)

        build_topo(self)
        self.grad = 1
        for node in reversed(topo):
            node._backward()
            

    
        


from graphviz import Digraph

def trace(root):
    nodes, edges = set(),set()
    def build(v):
        if v not in nodes:
            nodes.add(v)
            for child in v._prev:
                edges.add((child,v))
                build(child)
    build(root)
    return nodes,edges

def draw_dot(root):
    dot = Digraph(format='svg',graph_attr={'rankdir':'LR'})

    nodes, edges = trace(root)
    for n in nodes:
        uid = str(id(n))
        dot.node(name = uid,label = "{ %s | data %.4f| grad %.4f}"%(n.label,n.data,n.grad),shape='record')
        if n._op:
            dot.node(name = uid+n._op,label = n._op)
            dot.edge(uid + n._op,uid)
    for n1, n2 in edges:
        dot.edge(str(id(n1)),str(id(n2))+n2._op)

    return dot
                


x1 = Value(1.0,label='x1')
w1 = Value(0.0,label = 'w1')

x2 = Value(2.0,label = 'x2')
w2 = Value(-3.0,label = 'w2')

b  = Value(6.8814,label = 'b')

x1w1 = x1*w1
x1w1.label = 'x1w1'
x2w2 = x2*w2
x2w2.label = 'x2w2'

x1w1x2w2 = x1w1+x2w2
x1w1x2w2.label = 'x1w1x2w2'

n = x1w1x2w2 + b
n.label = 'n'
e = (2*n).exp()
L = (e-1)/(e+1)
L.label = 'L'


draw_dot(L)


a = Value(4.0)
b = Value(2.2)
a+4


L.backward()
draw_dot(L)


import random


class Neuron:
    def __init__(self,nin):
        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]
        self.b = Value(random.uniform(-1,1))

    def __call__(self,x):
        act = sum((wi*xi for wi,xi in zip(self.w,x)),self.b)
        out = act.tanh()
        return out

    def parameters(self):
        return [weight for weight in self.w] + [self.b]

class Layer:
    def __init__(self,nin,nout):
        self.neurons = [Neuron(nin) for _ in range(nout)]

    def __call__(self,x):
        outs = [n(x) for n in self.neurons]
        return outs[0] if len(outs) == 1 else outs

    def parameters(self):
        return [i for n in self.neurons for i in n.parameters()]

class MLP:
    def __init__(self,nin,nouts):
        sz = [nin] + nouts
        self.layers = [Layer(sz[i],sz[i+1]) for i in range(len(nouts))]
        
    def __call__(self,x):
        for layer in self.layers:
            x = layer(x)

        return x 

    def parameters(self):
        return [i for l in self.layers for i in l.parameters()]

  


from sklearn import datasets
from sklearn.preprocessing import LabelEncoder

# Load the Iris dataset
iris = datasets.load_iris()

# Extract features (X) and target (y)
X = iris.data
y = iris.target

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Label encode the target values
y_encoded = label_encoder.fit_transform(y)

# Display the encoded labels
print("Encoded labels:", y_encoded)



from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

# Scale the feature data (X) to the range [0, 1]
X_scaled = scaler.fit_transform(X)

# Display the scaled features
print("Scaled features (Min-Max):")
print(X_scaled)


import numpy as np
y_min = np.min(y)
y_max = np.max(y)

# Apply Min-Max Scaling to transform values to the range -1 to 1
y_scaled = 2 * (y - y_min) / (y_max - y_min) - 1

# Display the scaled values
print("Scaled y values:", y_scaled)
y_scaled = y_scaled.tolist()
type(y_scaled)


n = MLP(4,[4,4,1])
y_pred = [n(x) for x in X_scaled]
n.parameters()


for k in range(10):
    for i in n.parameters():
        i.grad = 0.0
    
    y_pred = [n(x) for x in X_scaled]
    loss = sum((y_scaled - y_pred)**2 for y_scaled,y_pred in zip(y_scaled,y_pred))
    loss.backward()
    for i in n.parameters():
        i.data += -0.1*i.grad

    print(loss)



y_pred 



